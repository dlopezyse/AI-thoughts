# AI-thoughts ü§ñ
A repository of great thoughts about Artificial Intelligence

### Bengio, Yoshua
- Instead of learning separately from images and videos on one hand, and from text on the other hand, we need to do a better job of jointly learning about language and about the world to which it refers, so that both sides can help each other. [Source](https://www.youtube.com/watch?v=azOmzumh0vQ)
- I don‚Äôt think that having more depth in the artificial neural networks (e.g. instead of a hundred layers we have ten thousand) is going to solve our learning problem. Engineers, companies, labs and grad students will continue to tune architectures and explore tweaks to make the current state of the art slightly better, but I don‚Äôt think that‚Äôs going to be nearly enough. I think we need some fairly drastic changes in the way that we are considering learning, to achieve the goal that these learners actually understand in a deep way the environment in which they are observing and acting. [Source](https://www.youtube.com/watch?v=azOmzumh0vQ)
- Our state of the art deep learning methods fail to learn models that understand even very simple environments. Instead of what humans might need just dozens of examples, these things will need millions for very simple tasks. [Source](https://www.youtube.com/watch?v=azOmzumh0vQ)
- I think there‚Äôs an opportunity for academics to do really important research to advance the state of the art in training frameworks, learning models, and agent learning, in even simple environments that are synthetic, seem trivial, but in which current machine learning fails. [Source](https://www.youtube.com/watch?v=azOmzumh0vQ)
- For machines, the hardest part of any conversation is everything to do with the non linguistic knowledge, which implicitly you need in order to make sense of sentences (e.g. sentences that are semantically ambiguous). You need to understand enough about the world in order to interpret properly those sentences. I think these are interesting challenges for Machine Learning because they point in the direction of building systems that both understand how the world works and it‚Äôs causal relationships, and associate that knowledge with how to express it in language for reading or writing. [Source](https://www.youtube.com/watch?v=azOmzumh0vQ)

### Hinton, Geoffrey 
- I recently realized that the kind of digital intelligence we‚Äôre developing might be a better form of intelligence than what biological brains have. I always used to think that deep learning was trying to mimic the brain, but that it wasn‚Äôt as good as the brain and we could make it better by making it more like the brain. But now I think AI systems may be doing some things more efficiently than the brain. [Source](https://english.elpais.com/science-tech/2023-05-12/geoffrey-hinton-we-need-to-find-a-way-to-control-artificial-intelligence-before-its-too-late.html)
- With a digital system, you can have many, many copies of the exact same model of the world. These copies can work on different hardware. Thus, different copies can analyze different data. And all these copies can instantly know what the others have learned. They do this by sharing, but we cannot do that with the brain. Our minds have learned to function individually. [Source](https://english.elpais.com/science-tech/2023-05-12/geoffrey-hinton-we-need-to-find-a-way-to-control-artificial-intelligence-before-its-too-late.html)
- I think it's quite conceivable that humanity is just a passing phase in the evolution of intelligence. We created an immortal form of digital intelligence that might be shut off on one machine to bring it under control. But it could easily be brought back to life on another machine if given the proper instructions. And it may keep us around for a while to keep the power stations running. But after that, maybe not. So the good news is we figured out how to build beings that are immortal. But that immortality, is not for us. [Source](https://english.elpais.com/science-tech/2023-05-12/geoffrey-hinton-we-need-to-find-a-way-to-control-artificial-intelligence-before-its-too-late.html)

### Koch, Christof
- Consciousness is any experience. It feels like something to be bad, to be an American, or to be angry, sad or in love, or to have pain. And that is what experience is. It could be as mundane as sitting on a chair, or could be as exalted as having a mystical moment in deep meditation. [Source](https://www.youtube.com/watch?v=piHkfmeU7Wo)
- There is a concept of intelligence (natural or artificial), and there is a concept of consciousness experience (natural or artificial). And those are very different things. Historically we associate consciousness with intelligence, but now we confront a world where we are beginning to engineer intelligence, and it‚Äôs radically unclear whether that intelligence we‚Äôre engineering has anything to do with consciousness, and whether it can experience anything. [Source](https://www.youtube.com/watch?v=piHkfmeU7Wo)
- Intelligence is about function. It‚Äôs about adaptation to new environments, being able to learn, quickly understand, and what will happen next. Consciousness is not about function. It‚Äôs about being. [Source](https://www.youtube.com/watch?v=piHkfmeU7Wo)
- Why is consciousness a hard problem? Because it‚Äôs subjective. Only I have it for only I know. I‚Äôve direct experience of my own consciousness. I don‚Äôt have experience on your consciousness. [Source](https://www.youtube.com/watch?v=piHkfmeU7Wo)
- In humans, intelligence and consciousness go hand by hand. In artificial systems, particularly digital machines, they don‚Äôt go together. Systems may simulate the behaviours associated with consciousness, but simulating is not the same as having conscious experiences. Just like it doesn‚Äôt get wet inside a computer when it simulates a weather storm, in order to have artificial consciousness you have to give it the same causal power as a human brain. [Source](https://www.youtube.com/watch?v=piHkfmeU7Wo)

### Pinker, Steven
- Is there any difference between the human neural network and the ones we are building in AI? I think there is overlap, but also that there are some big differences. Current artificial neural networks called deep learning systems are in reality not all that deep. They are very good at extracting high order statistical regularities, but most of the systems don‚Äôt have a semantic level: a level of actual understanding of who did what to whom, why, where, how things work, what causes what else. [Source](https://www.youtube.com/watch?v=epQxfSp-rdU)
- The goal of making an artificial system that is exactly like the human brain is a goal that no one is going to pursue to the bitter end, because if you want tools that do thing better than humans, you‚Äôre not going to care whether it does something like humans. Why take humans as benchmarks? [Source](https://www.youtube.com/watch?v=epQxfSp-rdU)
- Goals are external to the means to attain the goals: if we don‚Äôt design an AI system to maximize dominance, then it won‚Äôt maximize dominance. It‚Äôs just that we are so familiar with Homo Sapiens when these two traits come bundled together, particularly in men, that we are apt to confuse high intelligence with a will to power. But that‚Äôs just an error. [Source](https://www.youtube.com/watch?v=epQxfSp-rdU)
- Other fear is that we‚Äôll be collateral damage: that we will give an AI a goal like curing cancer, and it will turn us into guinea pigs for lethal experiments. I think these kind of scenarios are self-defeating. First of all, they assume that we‚Äôre going to be so brilliant that we can design an AI that can cure cancer, but so stupid that we don‚Äôt specify what we mean by curing cancer in enough detail that it won‚Äôt kill us in the process. And it assumes that the system will be so smart that it can cure cancer, but so idiotic that it doesn‚Äôt figure out that what we mean by curing cancer is not killing everyone. I think this value alignment problem is based on a misconception. The code of engineering is: you don‚Äôt implement a system with massive control before testing it. [Source](https://www.youtube.com/watch?v=epQxfSp-rdU)

### Russell, Stuart
- We have to be certain that the purpose we put into the machine is the purpose we really desire, and the problem is, we can‚Äôt do that right. In practice, it‚Äôs extremely unlikely that we could specify correctly in advance the full range of concerns of humanity. [Source](https://www.youtube.com/watch?v=KsZI5oXBC0k)
- What we need to do is to get away from this idea that you build an optimizing machine and you put the objective into it, because if it‚Äôs possible that you might put in a wrong objective, that means that the machine should never take an objective that‚Äôs given as gospel truth. Because once it takes the objective as gospel truth, it believes that whatever actions it‚Äôs taking in pursuit of that objective are the correct things to do. And this is not restricted to AI: in statistics you minimize a loss function, in control theory you minimize a cost function, in operations research you maximize a reward function, and so on. In all these disciplines this is how we conceive the problem, and it‚Äôs the wrong problem, because we can‚Äôt specify with certainty the correct objective. [Source](https://www.youtube.com/watch?v=KsZI5oXBC0k)
- We need uncertainty. We need the machine to be uncertain about what it is that is supposed to be maximizing. A machine that‚Äôs uncertain is going to be differential to us: if we say don‚Äôt do that, now the machine learnt something more about our true objectives because something that it ‚Äúthought‚Äù was reasonable in pursuit of our objectives turned out not to be so. It‚Äôs going to differ because it wants to be doing what we really want. [Source](https://www.youtube.com/watch?v=KsZI5oXBC0k)
- It‚Äôs a different kind of AI when you take away this idea that the objective is known, and you get a more complicated problem because now the interaction with the human becomes part of the problem. By making choices, the human is giving the machine more information about the true objective, and that information helps the machine to achieve the objective better. That means you‚Äôre mostly dealing with game theoretic problems where you‚Äôve got the machine and the human, and they‚Äôre coupled together, rather than the machine going off by itself with a fixed objective. [Source](https://www.youtube.com/watch?v=KsZI5oXBC0k)

### Schmidhuber, J√ºrgen
- There are significant differences between the way systems can learn. Let‚Äôs take an example of a deep neural network that has learnt to classify images, trained on 100 different databases of images. Now a new database comes along, and you want to quickly learn the new thing as well. One simple way of doing that is taking the network which already knows 100 types of databases, take its top layer, and retrain that using the new label data that you have in the new image database. Then, it quickly can learn that too. The neural network has already learned so much about computer vision that it can reuse that knowledge to solve the new task, except that you need a little bit of adjustment on the top. That is transfer learning. [Source](https://www.youtube.com/watch?v=3FIo6evmweo)
- On the other hand, true meta-learning is about having the learning algorithm itself opened to introspection by the system that is using it. And also opened to modification, such that the learning system has an opportunity to modify any part of the learning algorithm, and then evaluate the consequences of that modification, learning from that to create a better learning algorithm, recursively. [Source](https://www.youtube.com/watch?v=3FIo6evmweo)
- I think that in the near future we will have (for the first time) robots that learn like kids. Robots that by seeing and hearing us guiding them, will try to do something with their own actuators, which will be different from ours but they will understand the difference. They will learn to imitate us, but not in the supervised way where a teacher gives target signals for all muscles all the time. They will learn high level imitation, where they first have to imitate us and then interpret these additional noises coming from my mouth (voice) as helpful signals to do tasks better. Then, and by their selves, will come up with faster and more efficient ways of doing the same things that we taught them. At the moment this is not possible, but we already see how we are going to get there. To the extent that this works economically, it‚Äôs going to change everything. Almost all our production will be affected, and a much bigger AI wave than the one we are witnessing is coming: an era of active machines that shape data through the actions they execute, learning to do that in a good way. [Source](https://www.youtube.com/watch?v=3FIo6evmweo)

###  Tegmark, Max
- When we build machines, we normally build them with some kind of goal: win this chess game, drive this car safely, or whatever. As soon as you put a goal into a machine, and especially if it‚Äôs kind of an open-ended goal and the machine is very intelligent, it will break that down into a bunch of sub goals. One of those goals will almost always be self-preservation, because if it breaks or dies in the process it‚Äôs not going to accomplish the goal. Similarly, if you give any kind of ambitious goal to an Artificial General Intelligence (AGI), it‚Äôs very likely it will want to acquire more resources so it can do that better, and it‚Äôs exactly from those sort of sub goals we might not have intended that some of the concerns about AGI safety come from. You give AGI some goal which seems completely harmless and then, before you realize, it‚Äôs also trying to do these other things which you didn‚Äôt want it to do. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
- Right now we have machines that are much better than us at some very narrow tasks like multiplying large numbers fast, memorizing large databases, playing chess, playing GO, and soon driving cars. But there is still no machine that can match a human child in general intelligence. AGI is, by its very definition, the quest to build a machine that can do everything as well as we can. If that ever happens, I think it‚Äôs going to be the biggest transition in the history of life on Earth. But the really big change doesn‚Äôt come exactly the moment they are better than us at everything. It‚Äôs actually earlier. First there are big changes when they start becoming better than us at doing most of the jobs that we do, because that takes away much of the demand for human labor. And then, the really whopping change comes when they become better than us at AI research. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
- Right now the timescale of AI research is limited by the human research and development cycle (typically of years). But once we replace engineers with equivalent pieces of software, then there‚Äôs no reason to think about years, and cycles can be much faster. The timescale of future progress in AI, and also all of science and technology, will be driven by machines. The really interesting moment is when AI gets better than us at AI programming, so that they can, if they want to, get better than us at anything. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
- My hunch is that we‚Äôre going to understand how to build AGI before we fully understand how our brains work, just like we understood how to build flying machines long before we were able to build a mechanical bird. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
- Regarding human-machine objective alignment, we should start with kindergarten ethics (that pretty everybody agrees on), and put that into our machines now. For example, anyone who builds passenger aircrafts wants to never, under any circumstance, fly into a building or a mountain. When Andreas Lubitz (the German Wings pilot) flew his passenger jet into the Alps killing over a hundred people in 2015, he just told the autopilot to do it. And even though it had the GPS maps, the computer accepted it. We should take those very basic values where the problem is not that we don‚Äôt agree, the problem is just we‚Äôve been to lazy to put it into our machines and make sure that from now on airplanes refuse to do something like that. Instead, go into safe mode, maybe lock the cockpit door and go directly to the airport. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
