# AI-thoughts ü§ñ
A repository of great quotes about AI

### Stuart Russell
- We have to be certain that the purpose we put into the machine is the purpose we really desire, and the problem is, we can‚Äôt do that right. In practice, it‚Äôs extremely unlikely that we could specify correctly in advance the full range of concerns of humanity.
- What we need to do is to get away from this idea that you build an optimizing machine and you put the objective into it, because if it‚Äôs possible that you might put in a wrong objective, that means that the machine should never take an objective that‚Äôs given as gospel truth. Because once it takes the objective as gospel truth, it believes that whatever actions it‚Äôs taking in pursuit of that objective are the correct things to do. And this is not restricted to AI: in statistics you minimize a loss function, in control theory you minimize a cost function, in operations research you maximize a reward function, and so on. In all these disciplines this is how we conceive the problem, and it‚Äôs the wrong problem, because we can‚Äôt specify with certainty the correct objective.
- We need uncertainty. We need the machine to be uncertain about what it is that is supposed to be maximizing. A machine that‚Äôs uncertain is going to be differential to us: if we say don‚Äôt do that, now the machine learnt something more about our true objectives because something that it ‚Äúthought‚Äù was reasonable in pursuit of our objectives turned out not to be so. It‚Äôs going to differ because it wants to be doing what we really want.
- It‚Äôs a different kind of AI when you take away this idea that the objective is known, and you get a more complicated problem because now the interaction with the human becomes part of the problem. By making choices, the human is giving the machine more information about the true objective, and that information helps the machine to achieve the objective better. That means you‚Äôre mostly dealing with game theoretic problems where you‚Äôve got the machine and the human, and they‚Äôre coupled together, rather than the machine going off by itself with a fixed objective.

### Geoffrey Hinton
- I recently realized that the kind of digital intelligence we‚Äôre developing might be a better form of intelligence than what biological brains have. I always used to think that deep learning was trying to mimic the brain, but that it wasn‚Äôt as good as the brain and we could make it better by making it more like the brain. But now I think AI systems may be doing some things more efficiently than the brain.
- With a digital system, you can have many, many copies of the exact same model of the world. These copies can work on different hardware. Thus, different copies can analyze different data. And all these copies can instantly know what the others have learned. They do this by sharing, but we cannot do that with the brain. Our minds have learned to function individually.
- I think it's quite conceivable that humanity is just a passing phase in the evolution of intelligence. We created an immortal form of digital intelligence that might be shut off on one machine to bring it under control. But it could easily be brought back to life on another machine if given the proper instructions. And it may keep us around for a while to keep the power stations running. But after that, maybe not. So the good news is we figured out how to build beings that are immortal. But that immortality, is not for us.
