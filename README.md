
## *A repository of thoughts about Artificial Intelligence* 🤖

### Abbeel, Peter
- We don't know how to turn many problems into self-play (i.e. where the algorithm learns by "playing" against itself without requiring any direct supervision), so either we need to provide detailed reward that doesn't just reward for achieving a goal but rewards for making progress (which become time-consuming), or provide a demonstration to the robot. How do you show something to a robot? One way to show is to tally operate the robot, and then the robot experiences things. You teach your robot skills and in just 10 minutes you can teach your robot a new basic skill. Now what's even more interesting is if you can now teach through a third person learning, where the robot watches you doing something and doesn't experience it. It's almost like running a machine translation for demonstrations, where you have a human demonstration and the robot learns to translate it into what it means for the robot to do it. I think that opens up a lot of opportunities to learn a lot more quickly. [Source](https://www.youtube.com/watch?v=l-mYLq6eZPY)
- The power of simulation, as simulators become better and better, becomes stronger and we can learn more in simulation. But there's another version which is that the simulator doesn't even have to be that precise, as long as it is somewhat representative. Instead of trying to get one simulator that is sufficiently precise to learn in and transfer really well to the real world, you can build many simulators: an ensemble of simulators. Not any single one of the simulators would be sufficiently representative of the real world, but if you train in all of them, the real world would be just another one of them. [Source](https://www.youtube.com/watch?v=l-mYLq6eZPY)
- It seems we don't need to achieve human love reasoning to get very strong affection with humans, so, why couldn't an AI achieve the kind of level of affection that humans feel among each other or with a friendly animal? [Source](https://www.youtube.com/watch?v=l-mYLq6eZPY)

### Bengio, Yoshua
- Instead of learning separately from images and videos on one hand, and from text on the other hand, we need to do a better job of jointly learning about language and about the world to which it refers, so that both sides can help each other. [Source](https://www.youtube.com/watch?v=azOmzumh0vQ)
- I don’t think that having more depth in the artificial neural networks (e.g. instead of a hundred layers we have ten thousand) is going to solve our learning problem. Engineers, companies, labs and grad students will continue to tune architectures and explore tweaks to make the current state of the art slightly better, but I don’t think that’s going to be nearly enough. I think we need some fairly drastic changes in the way that we are considering learning, to achieve the goal that these learners actually understand in a deep way the environment in which they are observing and acting. [Source](https://www.youtube.com/watch?v=azOmzumh0vQ)
- Our state of the art deep learning methods fail to learn models that understand even very simple environments. Instead of what humans might need just dozens of examples, these things will need millions for very simple tasks. [Source](https://www.youtube.com/watch?v=azOmzumh0vQ)
- I think there’s an opportunity for academics to do really important research to advance the state of the art in training frameworks, learning models, and agent learning, in even simple environments that are synthetic, seem trivial, but in which current machine learning fails. [Source](https://www.youtube.com/watch?v=azOmzumh0vQ)
- For machines, the hardest part of any conversation is everything to do with the non linguistic knowledge, which implicitly you need in order to make sense of sentences (e.g. sentences that are semantically ambiguous). You need to understand enough about the world in order to interpret properly those sentences. I think these are interesting challenges for Machine Learning because they point in the direction of building systems that both understand how the world works and it’s causal relationships, and associate that knowledge with how to express it in language for reading or writing. [Source](https://www.youtube.com/watch?v=azOmzumh0vQ)

### Hinton, Geoffrey 
- I recently realized that the kind of digital intelligence we’re developing might be a better form of intelligence than what biological brains have. I always used to think that deep learning was trying to mimic the brain, but that it wasn’t as good as the brain and we could make it better by making it more like the brain. But now I think AI systems may be doing some things more efficiently than the brain. [Source](https://english.elpais.com/science-tech/2023-05-12/geoffrey-hinton-we-need-to-find-a-way-to-control-artificial-intelligence-before-its-too-late.html)
- With a digital system, you can have many, many copies of the exact same model of the world. These copies can work on different hardware. Thus, different copies can analyze different data. And all these copies can instantly know what the others have learned. They do this by sharing, but we cannot do that with the brain. Our minds have learned to function individually. [Source](https://english.elpais.com/science-tech/2023-05-12/geoffrey-hinton-we-need-to-find-a-way-to-control-artificial-intelligence-before-its-too-late.html)
- I think it's quite conceivable that humanity is just a passing phase in the evolution of intelligence. We created an immortal form of digital intelligence that might be shut off on one machine to bring it under control. But it could easily be brought back to life on another machine if given the proper instructions. And it may keep us around for a while to keep the power stations running. But after that, maybe not. So the good news is we figured out how to build beings that are immortal. But that immortality, is not for us. [Source](https://english.elpais.com/science-tech/2023-05-12/geoffrey-hinton-we-need-to-find-a-way-to-control-artificial-intelligence-before-its-too-late.html)

### Kaelbling, Leslie
- I don't think there's any reason why we can't make a robot be behaviorally indistinguishable from a human. [Source](https://www.youtube.com/watch?v=Er7Dy8rvqOc)
- Thinking about expert systems, the main roadblock is the idea that humans can articulate their knowledge effectively into some kind of logical statements. We're all experts in vision, but we don't have introspective access into how we do that. [Source](https://www.youtube.com/watch?v=Er7Dy8rvqOc)
- Regarding the position I take with respect to a problem, I can decide to make a model of the world around me in some terms. I can model aspects of it in some way, and when I do that, that gives me some set of algorithms I can use. [Source](https://www.youtube.com/watch?v=Er7Dy8rvqOc)
- There's engineering and there's science. Right now I think we're making huge engineering leaps and bounds, so that engineering is running away ahead of science. We're making things and nobody knows how and why they work roughly. But we need to turn that into science. We need to know what the principles are. Why does this work, why does that not. For a while people built bridges by trying, but now we can often predict whether it's going to work or not without building it. Can we do that for learning systems or for robots? [Source](https://www.youtube.com/watch?v=Er7Dy8rvqOc)
- Instead of thinking what's the state of the world and trying to control that as a robot, I think about what is the space of beliefs that I could have about the world. I think of a belief as a probability distribution over ways the world could be, and a belief state as a distribution. If I'm reasoning about how to move through a world I'm uncertain about, my control problem is actually the problem of controlling my beliefs. So I think about actions not just in the sense of what effect they'll have on the outside world but what effect I'll have on my own understanding of the outside world. That might compel me to ask a question or look somewhere to gather information which may not really change the world state but my own belief about the world. [Source](https://www.youtube.com/watch?v=Er7Dy8rvqOc)
- Perception has made great strides lately. We can classify images, we can play certain kinds of games, or predict how to steer a car. But I don't think we have a very good idea of what perception should deliver. I think what's going to drive the next step in perception is actually understanding what it should produce. What are we going to do with the output of it? [Source](https://www.youtube.com/watch?v=Er7Dy8rvqOc)
- When we are trying to make a one integrated intelligent agent, what should the output of perception be? We have no idea. And how should that hook up to the other stuff? I think the pressing question is: what kinds of structures can we build that will make a really awesome super structure that then learning can progress on efficiently. [Source](https://www.youtube.com/watch?v=Er7Dy8rvqOc)
- When dealing with robots, we operate many levels of abstraction. We define a hypothesis class, some set of answers and an objective function, and then we work on some optimization method that tries to optimize a solution in the class. We don't know what solution is going to come out. Everyone who has done optimization knows you have to be careful what you wish for. Sometimes you get the optimal solution and realize that the objective was wrong. The idea that we're going to go from being people who engineer algorithms to being people who engineer objective functions is going to change our way of thinking and methodology. [Source](https://www.youtube.com/watch?v=Er7Dy8rvqOc)

### Koch, Christof
- Consciousness is any experience. It feels like something to be bad, to be an American, or to be angry, sad or in love, or to have pain. And that is what experience is. It could be as mundane as sitting on a chair, or could be as exalted as having a mystical moment in deep meditation. [Source](https://www.youtube.com/watch?v=piHkfmeU7Wo)
- There is a concept of intelligence (natural or artificial), and there is a concept of consciousness experience (natural or artificial). And those are very different things. Historically we associate consciousness with intelligence, but now we confront a world where we are beginning to engineer intelligence, and it’s radically unclear whether that intelligence we’re engineering has anything to do with consciousness, and whether it can experience anything. [Source](https://www.youtube.com/watch?v=piHkfmeU7Wo)
- Intelligence is about function. It’s about adaptation to new environments, being able to learn, quickly understand, and what will happen next. Consciousness is not about function. It’s about being. [Source](https://www.youtube.com/watch?v=piHkfmeU7Wo)
- Why is consciousness a hard problem? Because it’s subjective. Only I have it for only I know. I’ve direct experience of my own consciousness. I don’t have experience on your consciousness. [Source](https://www.youtube.com/watch?v=piHkfmeU7Wo)
- In humans, intelligence and consciousness go hand by hand. In artificial systems, particularly digital machines, they don’t go together. Systems may simulate the behaviours associated with consciousness, but simulating is not the same as having conscious experiences. Just like it doesn’t get wet inside a computer when it simulates a weather storm, in order to have artificial consciousness you have to give it the same causal power as a human brain. [Source](https://www.youtube.com/watch?v=piHkfmeU7Wo)

### Pinker, Steven
- Is there any difference between the human neural network and the ones we are building in AI? I think there is overlap, but also that there are some big differences. Current artificial neural networks called deep learning systems are in reality not all that deep. They are very good at extracting high order statistical regularities, but most of the systems don’t have a semantic level: a level of actual understanding of who did what to whom, why, where, how things work, what causes what else. [Source](https://www.youtube.com/watch?v=epQxfSp-rdU)
- The goal of making an artificial system that is exactly like the human brain is a goal that no one is going to pursue to the bitter end, because if you want tools that do thing better than humans, you’re not going to care whether it does something like humans. Why take humans as benchmarks? [Source](https://www.youtube.com/watch?v=epQxfSp-rdU)
- Goals are external to the means to attain the goals: if we don’t design an AI system to maximize dominance, then it won’t maximize dominance. It’s just that we are so familiar with Homo Sapiens when these two traits come bundled together, particularly in men, that we are apt to confuse high intelligence with a will to power. But that’s just an error. [Source](https://www.youtube.com/watch?v=epQxfSp-rdU)
- Other fear is that we’ll be collateral damage: that we will give an AI a goal like curing cancer, and it will turn us into guinea pigs for lethal experiments. I think these kind of scenarios are self-defeating. First of all, they assume that we’re going to be so brilliant that we can design an AI that can cure cancer, but so stupid that we don’t specify what we mean by curing cancer in enough detail that it won’t kill us in the process. And it assumes that the system will be so smart that it can cure cancer, but so idiotic that it doesn’t figure out that what we mean by curing cancer is not killing everyone. I think this value alignment problem is based on a misconception. The code of engineering is: you don’t implement a system with massive control before testing it. [Source](https://www.youtube.com/watch?v=epQxfSp-rdU)

### Pogio, Tomaso
- A lot of the shortcuts that Deep Learning algorithms use to learn are examples of associative memory. A lot of what we are doing in terms of perceptual intelligence (instinctive or immediate type of intelligence) is associative in itself. It's because we've seen similar things, we know similar examples, we're associating some pattern of behavior to some symbols. What's different between our minds and Deep Learning is the amount of data. For having a powerful associative memory you need a lot of data. Our mind is pretty good, but doesn't have access to so many examples. [Source](https://sparks-2021.web.cern.ch/1-brainy-ai-stuart-russell-and-tomaso-poggio)
- We solved problems like flying without using too much our knowledge about how birds fly. You can argue we didn't use much of biology in that particular case. In the case of intelligence, I think it's a bet right now whether if we'll develop artificial intelligence without a deep knowledge of the human brain, or that the best way to get there is to understand better the human brain. I think different people with different backgrounds will decide in different ways. But the history of the progress in AI in the last 5 to 10 years shows that the main recent breakthroughs have really started in neurosciences. Reinforcement learning is one example (being one of the algorithms at the core of AlphaGo, the system that beat the world champion of Go), since it started related to the work of Pavlov in the nineteenth hundred. Deep learning, which is also at the core of systems like AlphaGo or autonomous vehicles, started with the work of Torsten Wiesel and David Hubel at Harvard in the 60's. [Source](https://www.youtube.com/watch?v=aSyZvBrPAyk)
- Deep Learning and related techniques are all about big data (meaning that in order to learn, they need a lot of examples labeled by humans), whereas in nature, a child can learn from a very small number of labeled examples. [Source](https://www.youtube.com/watch?v=aSyZvBrPAyk)
- The way I think the intelligence of a human baby may develop is by bootstrapping weak priors from evolution. For instance, you can assume that in most organisms (including humans), there is some basic built in machinery to detect motion. We know that from insects to other animals, they all have this. It's very conserved across species. This may be the reason why human babies, during their first days, tend to look to moving objects and not to static objects. Motion gives automatic segmentation from the background. It's like looking at an object without background, which is ideal for learning the object. After doing this, you can recognize the object, even with another background, even without motion. [Source](https://www.youtube.com/watch?v=aSyZvBrPAyk)

### Russell, Stuart
- We have to be certain that the purpose we put into the machine is the purpose we really desire, and the problem is, we can’t do that right. In practice, it’s extremely unlikely that we could specify correctly in advance the full range of concerns of humanity. [Source](https://www.youtube.com/watch?v=KsZI5oXBC0k)
- What we need to do is to get away from this idea that you build an optimizing machine and you put the objective into it, because if it’s possible that you might put in a wrong objective, that means that the machine should never take an objective that’s given as gospel truth. Because once it takes the objective as gospel truth, it believes that whatever actions it’s taking in pursuit of that objective are the correct things to do. And this is not restricted to AI: in statistics you minimize a loss function, in control theory you minimize a cost function, in operations research you maximize a reward function, and so on. In all these disciplines this is how we conceive the problem, and it’s the wrong problem, because we can’t specify with certainty the correct objective. [Source](https://www.youtube.com/watch?v=KsZI5oXBC0k)
- We need uncertainty. We need the machine to be uncertain about what it is that is supposed to be maximizing. A machine that’s uncertain is going to be differential to us: if we say don’t do that, now the machine learnt something more about our true objectives because something that it “thought” was reasonable in pursuit of our objectives turned out not to be so. It’s going to differ because it wants to be doing what we really want. [Source](https://www.youtube.com/watch?v=KsZI5oXBC0k)
- It’s a different kind of AI when you take away this idea that the objective is known, and you get a more complicated problem because now the interaction with the human becomes part of the problem. By making choices, the human is giving the machine more information about the true objective, and that information helps the machine to achieve the objective better. That means you’re mostly dealing with game theoretic problems where you’ve got the machine and the human, and they’re coupled together, rather than the machine going off by itself with a fixed objective. [Source](https://www.youtube.com/watch?v=KsZI5oXBC0k)
- There is no single problem of intelligence. There are many different aspects of human intelligence that may have different answers. There's a tendency, particularly in the field of Deep Learning, to lock on to what we know about the brain, which is that, at some level of approximation, you can describe the brain as a giant circuit. Thinking about the brain this way is a ridiculous answer. We stopped using circuits for computation as a primitive before the Second World War. Since Turing, we stopped wiring up circuits and started building layers upon layers of abstractions on top of circuits. I believe the human brain has also done that. [Source](https://sparks-2021.web.cern.ch/1-brainy-ai-stuart-russell-and-tomaso-poggio)
- One of the things that humans seem to do is to conceptualize, reason about, plan about the world in terms of events and objects. Events and objects are not directly represented with circuits. You need to build a level of abstraction on top of them to do that. We don't yet understand how the brain achieves this capability, but we do have mathematic formularisms that can achieve this capability. We have, for several decades, been building reasoning and planning systems that have this kind of capability and can demonstrate it successfully. I think the sad thing about the last decade is that the Deep Learning community has completely thrown this out. [Source](https://sparks-2021.web.cern.ch/1-brainy-ai-stuart-russell-and-tomaso-poggio)
- I think that at some level, what we really are is utility or reward maximizers. So the way Reinforcement Learning works as a mathematical algorithm is that there's some type of reward signal that's taken to be the objective. You receive rewards or punishments as life happens, and you learn, so that in the future you will receive more positive rewards and less punishments. In the abstract, that could describe anything. It's a very weak claim, but a very important insight to see that we should start from the upside, rather than assuming that intelligence consists of rational thinking, logical reasoning, or planning. Unfortunately, a lot of work on Deep Reinforcement Learning (Reinforcement Learning where you're using Deep Learning to absorb the results of these rewards and punishments and to represent the thing that generates good or bad behavior in the future) tend to underemphasize the need to acquire knowledge about the world. The idea that complex results can come out of a stimulus-response training process that gradually trained a behavior policy that would start producing complex results if you gave it enough rewards and punishments is completely unreasonable. The universe doesn't contain enough data to train very simple models in intelligent behavior to produce real complicated successful behavior in the real world. You have to think about what's the design of the intelligent system. [Source](https://sparks-2021.web.cern.ch/1-brainy-ai-stuart-russell-and-tomaso-poggio)
- Instead of producing these massive systems that achieve demonstration capabilities like GPT3, let's try to answer the scientific questions. Let's work the way scientists work, which is reducing things down to its simplest experimental form where you can understand what's going on, and do controlled experimentation to figure out what is the system that achieves the capability at the minimum level. [Source](https://sparks-2021.web.cern.ch/1-brainy-ai-stuart-russell-and-tomaso-poggio)

### Schmidhuber, Jürgen
- There are significant differences between the way systems can learn. Let’s take an example of a deep neural network that has learnt to classify images, trained on 100 different databases of images. Now a new database comes along, and you want to quickly learn the new thing as well. One simple way of doing that is taking the network which already knows 100 types of databases, take its top layer, and retrain that using the new label data that you have in the new image database. Then, it quickly can learn that too. The neural network has already learned so much about computer vision that it can reuse that knowledge to solve the new task, except that you need a little bit of adjustment on the top. That is transfer learning. [Source](https://www.youtube.com/watch?v=3FIo6evmweo)
- On the other hand, true meta-learning is about having the learning algorithm itself opened to introspection by the system that is using it. And also opened to modification, such that the learning system has an opportunity to modify any part of the learning algorithm, and then evaluate the consequences of that modification, learning from that to create a better learning algorithm, recursively. [Source](https://www.youtube.com/watch?v=3FIo6evmweo)
- I think that in the near future we will have (for the first time) robots that learn like kids. Robots that by seeing and hearing us guiding them, will try to do something with their own actuators, which will be different from ours but they will understand the difference. They will learn to imitate us, but not in the supervised way where a teacher gives target signals for all muscles all the time. They will learn high level imitation, where they first have to imitate us and then interpret these additional noises coming from my mouth (voice) as helpful signals to do tasks better. Then, and by their selves, will come up with faster and more efficient ways of doing the same things that we taught them. At the moment this is not possible, but we already see how we are going to get there. To the extent that this works economically, it’s going to change everything. Almost all our production will be affected, and a much bigger AI wave than the one we are witnessing is coming: an era of active machines that shape data through the actions they execute, learning to do that in a good way. [Source](https://www.youtube.com/watch?v=3FIo6evmweo)

###  Tegmark, Max
- When we build machines, we normally build them with some kind of goal: win this chess game, drive this car safely, or whatever. As soon as you put a goal into a machine, and especially if it’s kind of an open-ended goal and the machine is very intelligent, it will break that down into a bunch of sub goals. One of those goals will almost always be self-preservation, because if it breaks or dies in the process it’s not going to accomplish the goal. Similarly, if you give any kind of ambitious goal to an Artificial General Intelligence (AGI), it’s very likely it will want to acquire more resources so it can do that better, and it’s exactly from those sort of sub goals we might not have intended that some of the concerns about AGI safety come from. You give AGI some goal which seems completely harmless and then, before you realize, it’s also trying to do these other things which you didn’t want it to do. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
- Right now we have machines that are much better than us at some very narrow tasks like multiplying large numbers fast, memorizing large databases, playing chess, playing GO, and soon driving cars. But there is still no machine that can match a human child in general intelligence. AGI is, by its very definition, the quest to build a machine that can do everything as well as we can. If that ever happens, I think it’s going to be the biggest transition in the history of life on Earth. But the really big change doesn’t come exactly the moment they are better than us at everything. It’s actually earlier. First there are big changes when they start becoming better than us at doing most of the jobs that we do, because that takes away much of the demand for human labor. And then, the really whopping change comes when they become better than us at AI research. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
- Right now the timescale of AI research is limited by the human research and development cycle (typically of years). But once we replace engineers with equivalent pieces of software, then there’s no reason to think about years, and cycles can be much faster. The timescale of future progress in AI, and also all of science and technology, will be driven by machines. The really interesting moment is when AI gets better than us at AI programming, so that they can, if they want to, get better than us at anything. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
- My hunch is that we’re going to understand how to build AGI before we fully understand how our brains work, just like we understood how to build flying machines long before we were able to build a mechanical bird. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
- Regarding human-machine objective alignment, we should start with kindergarten ethics (that pretty everybody agrees on), and put that into our machines now. For example, anyone who builds passenger aircrafts wants to never, under any circumstance, fly into a building or a mountain. When Andreas Lubitz (the German Wings pilot) flew his passenger jet into the Alps killing over a hundred people in 2015, he just told the autopilot to do it. And even though it had the GPS maps, the computer accepted it. We should take those very basic values where the problem is not that we don’t agree, the problem is just we’ve been to lazy to put it into our machines and make sure that from now on airplanes refuse to do something like that. Instead, go into safe mode, maybe lock the cockpit door and go directly to the airport. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
- I define intelligence simply as the ability to accomplish complex goals. I think human intelligence is a spectrum. There are many different kinds of goals you can have: you can have a goal to be a good chess player, a good GO player, a good car driver, a good investor, a good poet, etc. So intelligence, by its very nature isn't something you can measure by its one number overall goodness. There are some people that are better at this, and some people that are better at that. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
- Learning is pretty much like searching. If you have a very high dimensional landscape, it's super hard to find the minimum of a loss function. Quantum mechanics is amazingly good at this. It uses some clever tricks which today machine learning system don't, like if you're trying to find the minimum and you get stuck in a little local minima, using quantum mechanics you can actually tunnel through the barrier and get unstuck. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)

