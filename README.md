# AI-thoughts ü§ñ
A repository of great thoughts about Artificial Intelligence

### Hinton, Geoffrey 
- I recently realized that the kind of digital intelligence we‚Äôre developing might be a better form of intelligence than what biological brains have. I always used to think that deep learning was trying to mimic the brain, but that it wasn‚Äôt as good as the brain and we could make it better by making it more like the brain. But now I think AI systems may be doing some things more efficiently than the brain. [Source](https://english.elpais.com/science-tech/2023-05-12/geoffrey-hinton-we-need-to-find-a-way-to-control-artificial-intelligence-before-its-too-late.html)
- With a digital system, you can have many, many copies of the exact same model of the world. These copies can work on different hardware. Thus, different copies can analyze different data. And all these copies can instantly know what the others have learned. They do this by sharing, but we cannot do that with the brain. Our minds have learned to function individually. [Source](https://english.elpais.com/science-tech/2023-05-12/geoffrey-hinton-we-need-to-find-a-way-to-control-artificial-intelligence-before-its-too-late.html)
- I think it's quite conceivable that humanity is just a passing phase in the evolution of intelligence. We created an immortal form of digital intelligence that might be shut off on one machine to bring it under control. But it could easily be brought back to life on another machine if given the proper instructions. And it may keep us around for a while to keep the power stations running. But after that, maybe not. So the good news is we figured out how to build beings that are immortal. But that immortality, is not for us. [Source](https://english.elpais.com/science-tech/2023-05-12/geoffrey-hinton-we-need-to-find-a-way-to-control-artificial-intelligence-before-its-too-late.html)

### Pinker, Steven
- Is there any difference between the human neural network and the ones we are building in AI? I think there is overlap, but also that there are some big differences. Current artificial neural networks called deep learning systems are in reality not all that deep. They are very good at extracting high order statistical regularities, but most of the systems don‚Äôt have a semantic level: a level of actual understanding of who did what to whom, why, where, how things work, what causes what else. [Source](https://www.youtube.com/watch?v=epQxfSp-rdU)
- The goal of making an artificial system that is exactly like the human brain is a goal that no one is going to pursue to the bitter end, because if you want tools that do thing better than humans, you‚Äôre not going to care whether it does something like humans. Why take humans as benchmarks? [Source](https://www.youtube.com/watch?v=epQxfSp-rdU)
- Goals are external to the means to attain the goals: if we don‚Äôt design an AI system to maximize dominance, then it won‚Äôt maximize dominance. It‚Äôs just that we are so familiar with Homo Sapiens when these two traits come bundled together, particularly in men, that we are apt to confuse high intelligence with a will to power. But that‚Äôs just an error. [Source](https://www.youtube.com/watch?v=epQxfSp-rdU)
- Other fear is that we‚Äôll be collateral damage: that we will give an AI a goal like curing cancer, and it will turn us into guinea pigs for lethal experiments. I think these kind of scenarios are self-defeating. First of all, they assume that we‚Äôre going to be so brilliant that we can design an AI that can cure cancer, but so stupid that we don‚Äôt specify what we mean by curing cancer in enough detail that it won‚Äôt kill us in the process. And it assumes that the system will be so smart that it can cure cancer, but so idiotic that it doesn‚Äôt figure out that what we mean by curing cancer is not killing everyone. I think this value alignment problem is based on a misconception. The code of engineering is: you don‚Äôt implement a system with massive control before testing it. [Source](https://www.youtube.com/watch?v=epQxfSp-rdU)

### Russell, Stuart
- We have to be certain that the purpose we put into the machine is the purpose we really desire, and the problem is, we can‚Äôt do that right. In practice, it‚Äôs extremely unlikely that we could specify correctly in advance the full range of concerns of humanity. [Source](https://www.youtube.com/watch?v=KsZI5oXBC0k)
- What we need to do is to get away from this idea that you build an optimizing machine and you put the objective into it, because if it‚Äôs possible that you might put in a wrong objective, that means that the machine should never take an objective that‚Äôs given as gospel truth. Because once it takes the objective as gospel truth, it believes that whatever actions it‚Äôs taking in pursuit of that objective are the correct things to do. And this is not restricted to AI: in statistics you minimize a loss function, in control theory you minimize a cost function, in operations research you maximize a reward function, and so on. In all these disciplines this is how we conceive the problem, and it‚Äôs the wrong problem, because we can‚Äôt specify with certainty the correct objective. [Source](https://www.youtube.com/watch?v=KsZI5oXBC0k)
- We need uncertainty. We need the machine to be uncertain about what it is that is supposed to be maximizing. A machine that‚Äôs uncertain is going to be differential to us: if we say don‚Äôt do that, now the machine learnt something more about our true objectives because something that it ‚Äúthought‚Äù was reasonable in pursuit of our objectives turned out not to be so. It‚Äôs going to differ because it wants to be doing what we really want. [Source](https://www.youtube.com/watch?v=KsZI5oXBC0k)
- It‚Äôs a different kind of AI when you take away this idea that the objective is known, and you get a more complicated problem because now the interaction with the human becomes part of the problem. By making choices, the human is giving the machine more information about the true objective, and that information helps the machine to achieve the objective better. That means you‚Äôre mostly dealing with game theoretic problems where you‚Äôve got the machine and the human, and they‚Äôre coupled together, rather than the machine going off by itself with a fixed objective. [Source](https://www.youtube.com/watch?v=KsZI5oXBC0k)

###  Tegmark, Max
- When we build machines, we normally build them with some kind of goal: win this chess game, drive this car safely, or whatever. As soon as you put a goal into a machine, and especially if it‚Äôs kind of an open-ended goal and the machine is very intelligent, it will break that down into a bunch of sub goals. One of those goals will almost always be self-preservation, because if it breaks or dies in the process it‚Äôs not going to accomplish the goal. Similarly, if you give any kind of ambitious goal to an Artificial General Intelligence (AGI), it‚Äôs very likely it will want to acquire more resources so it can do that better, and it‚Äôs exactly from those sort of sub goals we might not have intended that some of the concerns about AGI safety come from. You give AGI some goal which seems completely harmless and then, before you realize, it‚Äôs also trying to do these other things which you didn‚Äôt want it to do. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
- Right now we have machines that are much better than us at some very narrow tasks like multiplying large numbers fast, memorizing large databases, playing chess, playing GO, and soon driving cars. But there is still no machine that can match a human child in general intelligence. AGI is, by its very definition, the quest to build a machine that can do everything as well as we can. If that ever happens, I think it‚Äôs going to be the biggest transition in the history of life on Earth. But the really big change doesn‚Äôt come exactly the moment they are better than us at everything. It‚Äôs actually earlier. First there are big changes when they start becoming better than us at doing most of the jobs that we do, because that takes away much of the demand for human labor. And then, the really whopping change comes when they become better than us at AI research. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
- Right now the timescale of AI research is limited by the human research and development cycle (typically of years). But once we replace engineers with equivalent pieces of software, then there‚Äôs no reason to think about years, and cycles can be much faster. The timescale of future progress in AI, and also all of science and technology, will be driven by machines. The really interesting moment is when AI gets better than us at AI programming, so that they can, if they want to, get better than us at anything. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
- My hunch is that we‚Äôre going to understand how to build AGI before we fully understand how our brains work, just like we understood how to build flying machines long before we were able to build a mechanical bird. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
- Regarding human-machine objective alignment, we should start with kindergarten ethics (that pretty everybody agrees on), and put that into our machines now. For example, anyone who builds passenger aircrafts wants to never, under any circumstance, fly into a building or a mountain. When Andreas Lubitz (the German Wings pilot) flew his passenger jet into the Alps killing over a hundred people in 2015, he just told the autopilot to do it. And even though it had the GPS maps, the computer accepted it. We should take those very basic values where the problem is not that we don‚Äôt agree, the problem is just we‚Äôve been to lazy to put it into our machines and make sure that from now on airplanes refuse to do something like that. Instead, go into safe mode, maybe lock the cockpit door and go directly to the airport. [Source](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
